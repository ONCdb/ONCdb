{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this notebook processes VizieR (and with some tweaking, non-VizieR) catalogs for input into oncdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup -- run all cells in this section upon notebook startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import ICRS, Galactic, FK4, FK5\n",
    "from astropy.coordinates import Angle, Latitude, Longitude\n",
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# progress meter for big loops\n",
    "# note: progress must go from 0 to 100 because reasons\n",
    "\n",
    "def progress_meter(progress):\n",
    "    sys.stdout.write(\"\\rloading... %.1f%%\" % progress)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to group sources\n",
    "\n",
    "def group_sources(onc_df, dist_crit):\n",
    "    # \"new source\" numbering starts at highest ACS number + 1\n",
    "    new_source = max(onc_df.loc[onc_df['catname'] == 'ACS', 'catID'].values) + 1\n",
    "\n",
    "    exclude = set()\n",
    "\n",
    "    for k in range(len(onc_df)):\n",
    "\n",
    "        if k not in exclude:\n",
    "\n",
    "            # find where dist < dist_crit\n",
    "            m = onc_df.loc[onc_df[str(k)] < dist_crit]\n",
    "\n",
    "            mindex = set(m[str(k)].index.tolist())\n",
    "\n",
    "            mindex_updated = set(m[str(k)].index.tolist())\n",
    "            \n",
    "            # initially set False to ensure it goes through the loop at least once\n",
    "            mindex_same = False\n",
    "\n",
    "            # keep adding match values until no new values are added\n",
    "            while mindex_same == False:\n",
    "                for x in mindex:\n",
    "                    y = onc_df.loc[onc_df[str(x)] < dist_crit]\n",
    "\n",
    "                    yindex = set(y[str(x)].index.tolist())\n",
    "\n",
    "                    mindex_updated.update(yindex)\n",
    "                \n",
    "                # drops out of loop if no new things are added\n",
    "                mindex_same = (mindex == mindex_updated)\n",
    "\n",
    "                mindex.update(mindex_updated)\n",
    "            \n",
    "            # if already grouped, don't need to do it again\n",
    "            exclude.update(mindex)\n",
    "\n",
    "            num_group = len(mindex)\n",
    "\n",
    "            match = onc_df.loc[mindex,['catname','catID']]\n",
    "\n",
    "            # check for multiple objects in same catalog (any duplicates will flag as True)\n",
    "            if match.duplicated(subset='catname',keep=False).any() == True:\n",
    "\n",
    "                onc_df.loc[mindex,'oncflag'] = 'd'\n",
    "            \n",
    "            # check for one-to-one matches between ACS sources and new_cat sources (when new_cat is not ACS)\n",
    "            elif (cat_info[0] != 'ACS') and\\\n",
    "            ('ACS' in match['catname'].values) and (cat_info[0] in match['catname'].values):\n",
    "\n",
    "                onc_df.loc[mindex,'oncflag'] = 'o'\n",
    "            \n",
    "            onc_df.loc[mindex,'oncflag'] += str(num_group)\n",
    "            \n",
    "            # use ACS number if it exists -- if multiple, use lowest\n",
    "            if ('ACS' in match['catname'].values) == True:\n",
    "                onc_df.loc[mindex,'oncID'] = min(match.loc[match['catname'] == 'ACS','catID'].values)\n",
    "\n",
    "            # otherwise give it a new number\n",
    "            else:\n",
    "                onc_df.loc[mindex,'oncID'] = new_source\n",
    "                new_source += 1\n",
    "\n",
    "            progress_meter(k*100./len(onc_df))\n",
    "    \n",
    "    print '\\n'\n",
    "    \n",
    "    # change id columns to ints (defaults to floats...)\n",
    "    onc_df.loc[:,'catID'] = onc_df.loc[:,'catID'].astype(int)\n",
    "    onc_df.loc[:,'oncID'] = onc_df.loc[:,'oncID'].astype(int)\n",
    "    \n",
    "    return onc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pw_dists(c_new, c_onc):\n",
    "\n",
    "    cross_dist = pd.DataFrame()\n",
    "    self_dist = pd.DataFrame()\n",
    "\n",
    "    for k in range(len(c_new)):\n",
    "\n",
    "        # sep between new_cat and existing oncdb objects\n",
    "        sep_cross = c_onc.separation(c_new[k]).arcsecond\n",
    "\n",
    "        # internal sep between new_cat objects\n",
    "        sep_self = c_new.separation(c_new[k]).arcsecond\n",
    "\n",
    "        cross_dist.loc[:,k] = sep_cross\n",
    "        self_dist.loc[:,k] = sep_self\n",
    "\n",
    "        progress_meter(k*100./len(c_new))\n",
    "\n",
    "    print '\\n'\n",
    "\n",
    "    # join existing pw dist df (onc_ex) with cross_dist & self_dist dfs\n",
    "\n",
    "    '''\n",
    "     -------------------------------------\n",
    "    |        onc_ex          | cross_dist |\n",
    "    |-------------------------------------|\n",
    "    | new_cat | cross_dist.T | self_dist  |\n",
    "     -------------------------------------\n",
    "    '''\n",
    "    \n",
    "    # offsetting indices to make it join properly\n",
    "    nc_join = new_cat.rename(index = lambda x: (x + len(onc_ex)), inplace=False)\n",
    "\n",
    "    cross_dist.rename(columns = lambda x: (x + len(onc_ex)), inplace=True)\n",
    "    self_dist.rename(columns = lambda x: (x + len(onc_ex)), inplace=True)\n",
    "    self_dist.rename(index = lambda x: (x + len(onc_ex)), inplace=True)\n",
    "    \n",
    "    # join\n",
    "    pw1 = onc_ex.join(cross_dist)\n",
    "    pw2 = nc_join.join(cross_dist.transpose().join(self_dist))\n",
    "\n",
    "    pw1.columns = pw1.columns.astype(str)\n",
    "    pw2.columns = pw2.columns.astype(str)\n",
    "\n",
    "    onc_pw = pd.concat([pw1,pw2], ignore_index=True)\n",
    "\n",
    "    return onc_pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_offsets(onc_iv, catname):\n",
    "    \n",
    "    # restrict to one-to-one matches, sort by oncID so that matches are paired\n",
    "    o2o_new = onc_iv.loc[(onc_iv['oncflag'] == 'o') & (onc_iv['catname'] == catname) ,:].sort_values('oncID')\n",
    "    o2o_acs = onc_iv.loc[onc_iv['oncID'].isin(o2o_new['oncID']) & (onc_iv['catname'] == 'ACS')].sort_values('oncID')\n",
    "    \n",
    "    # get coords\n",
    "    c_o2o_new = SkyCoord(o2o_new.loc[o2o_new['catname'] == catname,'_RAJ2000'],\\\n",
    "                         o2o_new.loc[o2o_new['catname'] == catname,'_DEJ2000'], unit='degree')\n",
    "    c_o2o_acs = SkyCoord(o2o_acs.loc[o2o_acs['catname'] == 'ACS','_RAJ2000'],\\\n",
    "                         o2o_acs.loc[o2o_acs['catname'] == 'ACS','_DEJ2000'], unit='degree')\n",
    "\n",
    "    print len(c_o2o_acs), 'one-to-one matches found'\n",
    "\n",
    "    delta_ra = []\n",
    "    delta_dec = []\n",
    "\n",
    "    for i in range(len(c_o2o_acs)):\n",
    "        # offsets FROM ACS TO new catalog\n",
    "        ri, di = c_o2o_acs[i].spherical_offsets_to(c_o2o_new[i])\n",
    "\n",
    "        delta_ra.append(ri.arcsecond)\n",
    "        delta_dec.append(di.arcsecond)\n",
    "\n",
    "        progress_meter(i*100./len(c_o2o_acs))\n",
    "\n",
    "    delta_ra = np.array(delta_ra)\n",
    "    delta_dec = np.array(delta_dec)\n",
    "\n",
    "    print '\\n'\n",
    "    \n",
    "    # fit a gaussian\n",
    "    mu_ra, std_ra = norm.fit(delta_ra)\n",
    "    mu_dec, std_dec = norm.fit(delta_dec)\n",
    "\n",
    "    print 'Delta RA (arcsec):', mu_ra\n",
    "    print 'Delta DEC (arcsec):', mu_dec\n",
    "    \n",
    "    return (delta_ra, delta_dec, mu_ra, mu_dec, std_ra, std_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file root path -- expects inputs to be in this directory, and writes outputs to this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = '/Users/alin/Documents/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check contents of oncdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shows which catalogs are in an oncdb input file, and how many objects they have\n",
    "\n",
    "# file to check (either pw_dist or db_input file)\n",
    "onc_curr_name = 'R2013+WFC3_db_input.txt'\n",
    "\n",
    "# ====\n",
    "\n",
    "onc_curr = pd.read_csv(root + onc_curr_name, sep='\\t', engine='python')\n",
    "\n",
    "print onc_curr['catname'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add new cat to existing oncdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL -- likely only used if catalog is not downloaded from VizieR\n",
    "\n",
    "# example of taking a whitespace-separated ascii table and turning it into the proper format\n",
    "\n",
    "# in this case, only the first three columns (index, RAdeg, and DEdeg) are used\n",
    "# np.genfromtxt and pd.read_csv both got confused by the flags because columns are only separated by whitespace\n",
    "\n",
    "# this is fine though -- catalog ID, RA (deg), and DEC (deg) are the bare minimum data required\n",
    "\n",
    "temp = np.genfromtxt(root + 'AASTEX_Slimtable_FINAL.txt', skip_header=26, usecols=(0,1,2))\n",
    "\n",
    "df = pd.DataFrame(temp, columns=['ONCwfc3', '_RAJ2000', '_DEJ2000'])\n",
    "\n",
    "df.to_csv(root + 'wfc3_minimal.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export directly from VizieR as 'tab-separated values' (tsv)\n",
    "# need to comment out the two lines between the header and the data (comment = #)\n",
    "\n",
    "# filename of existing pairwise dist matrix (csv)\n",
    "onc_ex_name = 'test_acs.csv'\n",
    "\n",
    "# filename for updated pairwise dist matrix (csv)\n",
    "onc_up_name = 'test_nicmos.csv'\n",
    "\n",
    "# catalog info as a tuple -- ('catname', 'filename', 'catID column name')\n",
    "cat_info = ('NICMOS', 'viz_nicmos.tsv', 'ONCnic3')\n",
    "\n",
    "'''\n",
    "('ACS', 'viz_acs.tsv', 'ONCacs')\n",
    "('WFPC2', 'viz_wfpc2.tsv', 'ONCpc2')\n",
    "('NICMOS', 'viz_nicmos.tsv', 'ONCnic3')\n",
    "('WFC3', 'wfc3_minimal.csv', 'ONCwfc3')\n",
    "'''\n",
    "\n",
    "# radius for xmatch (arcsec), pre- and post-shift\n",
    "dist_crit_pre = 1.\n",
    "dist_crit_post = 0.25\n",
    "\n",
    "# ====\n",
    "\n",
    "# if not VizieR standard format, may have to set options to get it to read in properly\n",
    "\n",
    "# for VizieR catalogs\n",
    "new_cat = pd.read_csv(root + cat_info[1], sep='\\t', comment='#', engine='python')\n",
    "\n",
    "# for wfc3\n",
    "# new_cat = pd.read_csv(root + cat_info[1], sep='\\t', engine='python')\n",
    "\n",
    "# ====\n",
    "\n",
    "new_cat = new_cat[[cat_info[2],'_RAJ2000','_DEJ2000']].groupby(cat_info[2]).agg(lambda x: np.mean(x))\n",
    "\n",
    "# new_cat.insert(0,'ra_corr', np.nan)\n",
    "# new_cat.insert(0,'dec_corr', np.nan)\n",
    "\n",
    "new_cat.insert(0,'catID', new_cat.index)\n",
    "new_cat.insert(0,'catname', cat_info[0])\n",
    "new_cat.insert(0,'oncID', np.nan)\n",
    "\n",
    "new_cat = new_cat.reset_index(drop=True)\n",
    "\n",
    "print 'new catalog', cat_info[0], 'has', len(new_cat), 'objects'\n",
    "\n",
    "onc_ex = pd.read_csv(root + onc_ex_name, engine='python')\n",
    "\n",
    "print 'existing oncdb has', len(onc_ex), 'objects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_new = SkyCoord(new_cat['_RAJ2000'], new_cat['_DEJ2000'], unit='degree')\n",
    "c_onc = SkyCoord(onc_ex['_RAJ2000'], onc_ex['_DEJ2000'], unit='degree')\n",
    "\n",
    "onc_iv = get_pw_dists(c_new, c_onc)\n",
    "\n",
    "onc_iv = group_sources(onc_iv, dist_crit_pre)\n",
    "\n",
    "delta_ra, delta_dec, mu_ra, mu_dec, std_ra, std_dec = find_offsets(onc_iv, cat_info[0])\n",
    "\n",
    "# plot histograms for ra/dec\n",
    "f, (ax_ra, ax_dec) = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "ax_ra.hist(delta_ra, bins=20, normed=True)\n",
    "ax_ra.set_xlim(-1,1)\n",
    "ax_ra.axvline(0, color='r')\n",
    "ax_ra.axvline(mu_ra, color='k', linestyle='dashed')\n",
    "ax_ra.set_xlabel('distance (arcsec)', fontsize=16)\n",
    "ax_ra.yaxis.set_visible(False)\n",
    "\n",
    "r = np.linspace(-1, 1, 100)\n",
    "ax_ra.plot(r, norm.pdf(r,mu_ra,std_ra), 'k')\n",
    "\n",
    "ax_dec.hist(delta_dec, bins=20, normed=True)\n",
    "ax_dec.set_xlim(-1,1)\n",
    "ax_dec.axvline(0, color='r')\n",
    "ax_dec.axvline(mu_dec, color='k', linestyle='dashed')\n",
    "ax_dec.set_xlabel('distance (arcsec)', fontsize=16)\n",
    "ax_dec.yaxis.set_visible(False)\n",
    "\n",
    "d = np.linspace(-1, 1, 100)\n",
    "ax_dec.plot(d, norm.pdf(d,mu_dec,std_dec), 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweak new_cat coords by linear offset\n",
    "# in future, do this with TweakReg, which also accounts for rotation/scaling\n",
    "# (see thread at https://github.com/spacetelescope/drizzlepac/issues/77)\n",
    "\n",
    "ra_v2 = new_cat.loc[:,'_RAJ2000'] - np.cos(new_cat.loc[:,'_DEJ2000']) * (mu_ra/3600.)\n",
    "dec_v2 = new_cat.loc[:,'_DEJ2000'] - (mu_dec/3600.)\n",
    "\n",
    "c_new_v2 = SkyCoord(ra_v2, dec_v2, unit='degree')\n",
    "\n",
    "onc_up = get_pw_dists(c_new_v2, c_onc)\n",
    "\n",
    "onc_up = group_sources(onc_up, dist_crit_post)\n",
    "\n",
    "delta_ra, delta_dec, mu_ra, mu_dec, std_ra, std_dec = find_offsets(onc_up, cat_info[0])\n",
    "\n",
    "# plot histograms for ra/dec\n",
    "f, (ax_ra, ax_dec) = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "ax_ra.hist(delta_ra, bins=8, normed=True)\n",
    "ax_ra.set_xlim(-1,1)\n",
    "ax_ra.axvline(0, color='r')\n",
    "ax_ra.axvline(mu_ra, color='k', linestyle='dashed')\n",
    "ax_ra.set_xlabel('distance (arcsec)', fontsize=16)\n",
    "ax_ra.yaxis.set_visible(False)\n",
    "\n",
    "r = np.linspace(-1, 1, 100)\n",
    "ax_ra.plot(r, norm.pdf(r,mu_ra,std_ra), 'k')\n",
    "\n",
    "ax_dec.hist(delta_dec, bins=8, normed=True)\n",
    "ax_dec.set_xlim(-1,1)\n",
    "ax_dec.axvline(0, color='r')\n",
    "ax_dec.axvline(mu_dec, color='k', linestyle='dashed')\n",
    "ax_dec.set_xlabel('distance (arcsec)', fontsize=16)\n",
    "ax_dec.yaxis.set_visible(False)\n",
    "\n",
    "d = np.linspace(-1, 1, 100)\n",
    "ax_dec.plot(d, norm.pdf(d,mu_dec,std_dec), 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onc_up.to_csv(root + onc_up_name, index=False)\n",
    "\n",
    "print 'saved to', onc_up_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create db input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename for pairwise dist matrix (csv)\n",
    "onc_pw_name = 'test_wfc3.csv'\n",
    "\n",
    "# filename for updated db input (txt)\n",
    "onc_db_name = 'test_db_input.txt'\n",
    "\n",
    "# ====\n",
    "\n",
    "onc_pw = pd.read_csv(root + onc_pw_name, engine='python')\n",
    "\n",
    "# only keep object info, not the entire pairwise matrix\n",
    "onc_db = onc_pw.loc[:,['oncID','oncflag','catname','catID','_RAJ2000','_DEJ2000']]\n",
    "\n",
    "# rename columns to fit the oncdbweb schema\n",
    "onc_db.rename(columns={'oncID':'id', 'oncflag':'comments', '_RAJ2000':'ra', '_DEJ2000':'dec'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onc_db.to_csv(root + onc_db_name, sep='\\t', index=False)\n",
    "\n",
    "print 'saved to', onc_db_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inserting oncIDs back into original catalogs -- this is not a very elegant method, but it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oncdb = pd.read_csv(root + 'R2013+WFC3_db_input.txt', sep='\\t', engine='python')\n",
    "\n",
    "onc_acs = oncdb.loc[oncdb['catname'] == 'ACS', ['id','catID']]\n",
    "onc_wfpc2 = oncdb.loc[oncdb['catname'] == 'WFPC2', ['id','catID']]\n",
    "onc_nicmos = oncdb.loc[oncdb['catname'] == 'NICMOS', ['id','catID']]\n",
    "\n",
    "onc_acs.rename(columns={'catID':'ONCacs'}, inplace=True)\n",
    "onc_wfpc2.rename(columns={'catID':'ONCpc2'}, inplace=True)\n",
    "onc_nicmos.rename(columns={'catID':'ONCnic3'}, inplace=True)\n",
    "\n",
    "ACS = pd.read_csv(root + 'acs_full.csv', engine='python')\n",
    "WFPC2 = pd.read_csv(root + 'wfpc2_full.csv', engine='python')\n",
    "NICMOS = pd.read_csv(root + 'nicmos_full.csv', engine='python')\n",
    "\n",
    "print len(onc_acs), len(ACS)\n",
    "print len(onc_wfpc2), len(WFPC2)\n",
    "print len(onc_nicmos), len(NICMOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onc_a = onc_acs.merge(ACS)\n",
    "onc_w = onc_wfpc2.merge(WFPC2)\n",
    "onc_n = onc_nicmos.merge(NICMOS)\n",
    "\n",
    "onc_a.to_csv(root + 'ACS_with_id.txt', sep='\\t', index=False)\n",
    "onc_w.to_csv(root + 'WFPC2_with_id.txt', sep='\\t', index=False)\n",
    "onc_n.to_csv(root + 'NICMOS_with_id.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial build -- should only be used for first catalog (ACS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_info = ('ACS', 'viz_acs.tsv', 'ONCacs')\n",
    "\n",
    "# radius for xmatch (arcsec)\n",
    "dist_crit = 0.25\n",
    "\n",
    "# filename for new pw_dist matrix\n",
    "onc_build_name = 'test_acs.csv'\n",
    "\n",
    "# ====\n",
    "\n",
    "# may have to set appropriate options to get it to read in properly\n",
    "\n",
    "acs = pd.read_csv(root + cat_info[1], sep='\\t', comment='#', engine='python')\n",
    "\n",
    "# ====\n",
    "    \n",
    "acs = acs[[cat_info[2],'_RAJ2000','_DEJ2000']].groupby(cat_info[2]).agg(lambda x: np.mean(x))\n",
    "\n",
    "# for correcting coords -- insert two more cols (ra_corr, dec_corr) that are same as ra and dec\n",
    "    \n",
    "acs.insert(0,'catID', acs.index)\n",
    "acs.insert(0,'catname', cat_info[0])\n",
    "acs.insert(0,'oncID', np.nan)\n",
    "\n",
    "acs = acs.reset_index(drop=True)\n",
    "\n",
    "print \"ACS has\", len(acs), \"objects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates df of pairwise distances for ACS objects\n",
    "\n",
    "# this step takes 5-10 minutes to run\n",
    "# (can probably be rewritten with multi-processing for a significant speed increase)\n",
    "\n",
    "c_acs = SkyCoord(acs['_RAJ2000'], acs['_DEJ2000'], unit='degree')\n",
    "\n",
    "build_dist = pd.DataFrame()\n",
    "\n",
    "for k in range(len(c_acs)):\n",
    "    \n",
    "    sep = c_acs.separation(c_acs[k]).arcsecond\n",
    "    \n",
    "    build_dist.loc[:,k] = sep\n",
    "    \n",
    "    progress_meter(k*100./len(c_acs))\n",
    "\n",
    "print '\\n'\n",
    "\n",
    "onc_build = pd.concat([acs, build_dist], axis=1)\n",
    "\n",
    "onc_build.columns = onc_build.columns.astype(str)\n",
    "onc_build.index = onc_build.index.astype(str)\n",
    "\n",
    "onc_build = group_sources(onc_build, dist_crit)\n",
    "\n",
    "onc_build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onc_build.to_csv(root + onc_build_name, index=False)\n",
    "\n",
    "print 'saved to', onc_build_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
