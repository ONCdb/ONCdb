{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy as ast\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import ICRS, Galactic, FK4, FK5\n",
    "from astropy.coordinates import Angle, Latitude, Longitude\n",
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# progress meter for big loops\n",
    "# note: progress must go from 0 to 100 because reasons\n",
    "\n",
    "def progress_meter(progress):\n",
    "    sys.stdout.write(\"\\rloading... %.1f%%\" % progress)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = '/Users/alin/Documents/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check contents of oncdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACS       3399\n",
      "NICMOS    2116\n",
      "WFPC2     1488\n",
      "Name: catname, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# shows which catalogs are in oncdb and how many sources they have\n",
    "\n",
    "# filename of db to be used\n",
    "onc_curr_name = 'test_cat.csv'\n",
    "\n",
    "# ====\n",
    "\n",
    "onc_curr = pd.read_csv(root + onc_curr_name, low_memory=False)\n",
    "\n",
    "print onc_curr['catname'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bash catalogs into right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# catalogs only *need* to contain catID, _RAJ2000 (deg), _DEJ2000 (deg)\n",
    "# repeat observations of a source (i.e. same catID) are compressed to a single entry by the groupby\n",
    "\n",
    "# catalogs to be used, as a list of tuples -- ('catname', 'filename (csv)', 'catID column name')\n",
    "\n",
    "cat_info = [('ACS', 'acs_full.csv', 'ONCacs'),\\\n",
    "            ('WFPC2', 'wfpc2_full.csv', 'ONCpc2'),\\\n",
    "            ('NICMOS', 'nicmos_full.csv', 'ONCnic3')]\n",
    "\n",
    "# ====\n",
    "\n",
    "cat_dict = dict()\n",
    "\n",
    "for i in range(len(cat_info)):\n",
    "    \n",
    "    cat = pd.read_csv(root + cat_info[i][1])\n",
    "    \n",
    "    # use mean or median?\n",
    "    cat_basic = cat[[cat_info[i][2],'_RAJ2000','_DEJ2000']].groupby(cat_info[i][2]).agg(lambda x: np.mean(x))\n",
    "    \n",
    "    cat_basic.insert(0,'catID', cat_basic.index)\n",
    "    cat_basic.insert(0,'catname', cat_info[i][0])\n",
    "    \n",
    "    cat_dict[i] = cat_basic\n",
    "\n",
    "new_cat = pd.concat(cat_dict, ignore_index=True)\n",
    "\n",
    "# new_cat.insert(0,'oncID',np.nan)\n",
    "\n",
    "print \"new_cat has\", len(new_cat), \"sources\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add new cat to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename of existing db\n",
    "onc_ex_name = 'test_build.csv'\n",
    "\n",
    "# filename for updated db\n",
    "onc_up_name = 'test_add.csv'\n",
    "\n",
    "# ====\n",
    "\n",
    "onc_ex = pd.read_csv(root + onc_ex_name)\n",
    "\n",
    "print 'new_cat has', len(new_cat), 'entries'\n",
    "print 'oncdb has', len(onc_ex), 'entries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_new = SkyCoord(new_cat['_RAJ2000'], new_cat['_DEJ2000'], unit='degree')\n",
    "c_onc = SkyCoord(onc_ex['_RAJ2000'], onc_ex['_DEJ2000'], unit='degree')\n",
    "\n",
    "cross_dist = pd.DataFrame()\n",
    "self_dist = pd.DataFrame()\n",
    "\n",
    "for k in range(len(c_new)):\n",
    "    \n",
    "    # sep between new_cat and existing oncdb sources\n",
    "    sep_cross = c_onc.separation(c_new[k]).arcsecond\n",
    "    \n",
    "    # internal sep between new_cat sources\n",
    "    sep_self = c_new.separation(c_new[k]).arcsecond\n",
    "    \n",
    "    cross_dist.loc[:,k] = sep_cross\n",
    "    self_dist.loc[:,k] = sep_self\n",
    "    \n",
    "    progress_meter(k*100./len(c_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join existing dist df with sep_cross & sep_self dfs\n",
    "# this looks (and probably is) stupid, but it works for now...\n",
    "\n",
    "nc_join = new_cat.rename(index = lambda x: (x + len(onc_ex)), inplace=False)\n",
    "\n",
    "cd_join = cross_dist.rename(columns = lambda x: (x + len(onc_ex)), inplace=False)\n",
    "sd_join = self_dist.rename(columns = lambda x: (x + len(onc_ex)), inplace=False)\n",
    "\n",
    "sd_join.rename(index = lambda x: (x + len(onc_ex)), inplace=True)\n",
    "\n",
    "temp1 = onc_ex.join(cd_join)\n",
    "temp2 = nc_join.join(cd_join.transpose().join(sd_join))\n",
    "\n",
    "temp1.columns = temp1.columns.astype(str)\n",
    "temp2.columns = temp2.columns.astype(str)\n",
    "\n",
    "# print (temp1.columns == temp2.columns).sum()\n",
    "\n",
    "onc_up = pd.concat([temp1,temp2], ignore_index=True)\n",
    "\n",
    "onc_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onc_up.to_csv(root + onc_up_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### completely build db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename for new db\n",
    "onc_build_name = 'test_cat.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates df of all pairwise distances\n",
    "\n",
    "# beware that this step takes a while -- e.g. 35 min with 7000 sources\n",
    "# (can probably be parallelized, but I don't know how)\n",
    "# consequently, advise against completely rebuilding db unless *really* necessary\n",
    "\n",
    "c_new = SkyCoord(new_cat['_RAJ2000'], new_cat['_DEJ2000'], unit='degree')\n",
    "\n",
    "build_dist = pd.DataFrame()\n",
    "\n",
    "for k in range(len(c_new)):\n",
    "    \n",
    "    sep = c_new.separation(c_new[k]).arcsecond\n",
    "    \n",
    "    build_dist.loc[:,k] = sep\n",
    "    \n",
    "    progress_meter(k*100./len(c_new))\n",
    "\n",
    "onc_build = pd.concat([new_cat, build_dist], axis=1)\n",
    "\n",
    "onc_build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onc_build.to_csv(root + onc_build_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oncdb has 7003 sources in 3 catalogs\n"
     ]
    }
   ],
   "source": [
    "# the db must be completely built (all catalogs added) before grouping sources\n",
    "\n",
    "# filename of db to be used\n",
    "onc_gs_name = 'test_cat.csv'\n",
    "\n",
    "# filename for db with sources cross-matched and grouped\n",
    "onc_out_name = 'onc_out.csv'\n",
    "\n",
    "# distance for xmatch (arcsec)\n",
    "dist_crit = 1.\n",
    "\n",
    "# whether to restrict to unique, non-ambiguous matches\n",
    "unique_only = False\n",
    "\n",
    "# ====\n",
    "\n",
    "onc_gs = pd.read_csv(root + onc_gs_name)\n",
    "\n",
    "onc_gs.insert(0,'oncflag','')\n",
    "\n",
    "onc_gs.insert(0,'oncID',np.nan)\n",
    "\n",
    "num_cats = len(onc_gs['catname'].value_counts())\n",
    "\n",
    "print 'oncdb has', len(onc_gs), 'sources in', num_cats, 'catalogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading... 100.0%"
     ]
    }
   ],
   "source": [
    "# new source numbering starts at highest ACS number + 1\n",
    "new_source = max(onc_gs.loc[onc_gs['catname'] == 'ACS', 'catID'].values) + 1\n",
    "\n",
    "exclude = set()\n",
    "\n",
    "for k in range(len(onc_gs)):\n",
    "    \n",
    "    if k not in exclude:\n",
    "        \n",
    "        # find where dist < dist_crit\n",
    "        m = onc_gs.loc[onc_gs[str(k)] < dist_crit]\n",
    "\n",
    "        mindex = set(m[str(k)].index.tolist())\n",
    "\n",
    "        mindex_updated = set(m[str(k)].index.tolist())\n",
    "\n",
    "        mindex_same = False\n",
    "\n",
    "        iter_count = 0\n",
    "\n",
    "        # print 'initial', mindex\n",
    "\n",
    "        # keep adding match values until no new values are added -- still in progress\n",
    "\n",
    "        while mindex_same == False:\n",
    "            for x in mindex:\n",
    "                y = onc_gs.loc[onc_gs[str(x)] < dist_crit]\n",
    "\n",
    "                yindex = set(y[str(x)].index.tolist())\n",
    "\n",
    "                # print 'new', yindex\n",
    "\n",
    "                mindex_updated.update(yindex)\n",
    "\n",
    "            # print 'mindex', mindex\n",
    "            # print 'updated', mindex_updated\n",
    "\n",
    "            mindex_same = (mindex == mindex_updated)\n",
    "\n",
    "            mindex.update(mindex_updated)\n",
    "\n",
    "            iter_count += 1\n",
    "\n",
    "        exclude.update(mindex)\n",
    "        \n",
    "        num_group = len(mindex)\n",
    "        '''\n",
    "        if iter_count > 1:\n",
    "            onc_gs.loc[mindex,'oncflag'] += 'i'\n",
    "        '''\n",
    "        match = onc_gs.loc[mindex,['catname','catID']]\n",
    "\n",
    "        # check for multiple sources in same catalog (any duplicates will flag as True)\n",
    "        if match.duplicated(subset='catname',keep=False).any() == True:\n",
    "\n",
    "            onc_gs.loc[mindex,'oncflag'] += 'd' + str(num_group)\n",
    "\n",
    "            # if only looking for uniques, skip assigning a number and continue on to the next source\n",
    "            if unique_only == True:\n",
    "                continue\n",
    "\n",
    "        # use ACS number if it exists -- if multiple, use lowest\n",
    "        if ('ACS' in match['catname'].values) == True:            \n",
    "            onc_gs.loc[mindex,'oncID'] = min(match.loc[match['catname'] == 'ACS','catID'].values)\n",
    "\n",
    "        # otherwise give it a new number\n",
    "        else:\n",
    "            onc_gs.loc[mindex,'oncID'] = new_source\n",
    "            new_source += 1\n",
    "\n",
    "        progress_meter(k*100./len(onc_gs))\n",
    "    \n",
    "if unique_only == True:\n",
    "    print '\\n', onc_gs['oncID'].count(), '/', len(onc_gs), 'sources non-ambiguously grouped'\n",
    "\n",
    "# print onc_gs['oncflag'].value_counts()\n",
    "\n",
    "# onc_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onc_gs.to_csv(root + onc_out_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d4    200\n",
      "d2    154\n",
      "d3    111\n",
      "d5     55\n",
      "d6     18\n",
      "d8      8\n",
      "Name: oncflag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "blah = onc_gs.loc[onc_gs['oncflag'] != '',['oncID','oncflag','catname','catID']]\n",
    "\n",
    "blah.groupby('oncID').first()\n",
    "\n",
    "print blah['oncflag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
